{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, pipeline\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.simple\", split=\"train\", cache_dir=\"data\")\n",
    "# wiki.save_to_disk(\"wiki\")\n",
    "\n",
    "wiki = load_from_disk(\"wiki\")\n",
    "\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])\n",
    "\n",
    "d = wiki.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    with open(output_filename, 'w') as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)\n",
    "\n",
    "dataset_to_text(d[\"train\"], \"train.txt\")\n",
    "dataset_to_text(d[\"test\"], \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "    \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "    ]\n",
    "\n",
    "file = [\"train.txt\"]\n",
    "vocab_size = 30_522\n",
    "max_length = 512\n",
    "truncate_longer_samples = True\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train(files=file, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "model_path = \"pretrained-bert\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "tokenizer.save_model(model_path)\n",
    "\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "    tokenizer_config = {\n",
    "        \"do_lower_case\": True,\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"model_max_length\": max_length,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"max_len\": max_length,\n",
    "    }\n",
    "    json.dump(tokenizer_config, f)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "print(tokenizer(\"Helloworld!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=max_length, padding=\"max_length\", return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(example):\n",
    "    return tokenizer(example[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)\n",
    "\n",
    "if truncate_longer_samples:\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    \n",
    "    result = {\n",
    "        k: [t[i:i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "if not truncate_longer_samples:\n",
    "    train_dataset = train_dataset.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=4,\n",
    "        desc=\"Grouping texts in chunks of length {}\".format(max_length),\n",
    "    )\n",
    "    test_dataset = test_dataset.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=4,\n",
    "        desc=\"Grouping texts in chunks of length {}\".format(max_length),\n",
    "    )\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(model_config)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=16,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the planet is the largest planet in the solar system. with confidence 0.10147172957658768\n",
      "the earth is the largest planet in the solar system. with confidence 0.039829738438129425\n",
      "the solar is the largest planet in the solar system. with confidence 0.03895225003361702\n",
      "the moon is the largest planet in the solar system. with confidence 0.03471800684928894\n",
      "the sun is the largest planet in the solar system. with confidence 0.024273451417684555\n",
      "==================================================\n",
      "the capital of france is a. with confidence 0.08450669050216675\n",
      "the capital of france is in. with confidence 0.040367018431425095\n",
      "the capital of france is capital. with confidence 0.030357573181390762\n",
      "the capital of france is an. with confidence 0.02068197913467884\n",
      "the capital of france is france. with confidence 0.016986673697829247\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "model_path = \"pretrained-bert\"\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-8000\"))\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "examples = [\n",
    "    \"The [MASK] is the largest planet in the solar system.\",\n",
    "    \"The capital of France is [MASK].\"\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    for prection in fill_mask(example):\n",
    "        print(f\"{prection['sequence']} with confidence {prection['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
